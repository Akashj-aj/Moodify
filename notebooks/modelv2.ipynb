{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef6a4e3-735f-479b-87d7-d0f95d4968f9",
   "metadata": {},
   "source": [
    "Model version V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df154bc6-7f5d-4423-8835-7a69c99db383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(1.0), 1: np.float64(1.0), 2: np.float64(1.0), 3: np.float64(1.0), 4: np.float64(1.0), 5: np.float64(1.0), 6: np.float64(1.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKASH J\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">802,944</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m802,944\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │             \u001b[38;5;34m903\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">897,415</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m897,415\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,967</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m896,967\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKASH J\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - accuracy: 0.2166 - loss: 2.1303\n",
      "Epoch 1: val_accuracy improved from None to 0.28058, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 597ms/step - accuracy: 0.2372 - loss: 1.9332 - val_accuracy: 0.2806 - val_loss: 1.8932 - learning_rate: 1.0000e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:54\u001b[0m 501ms/step - accuracy: 0.2969 - loss: 1.8264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKASH J\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.28058 to 0.28178, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.2969 - loss: 1.8264 - val_accuracy: 0.2818 - val_loss: 1.8857 - learning_rate: 1.0000e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550ms/step - accuracy: 0.2958 - loss: 1.7842\n",
      "Epoch 3: val_accuracy improved from 0.28178 to 0.44458, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 580ms/step - accuracy: 0.3003 - loss: 1.7692 - val_accuracy: 0.4446 - val_loss: 1.4914 - learning_rate: 1.0000e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 477ms/step - accuracy: 0.4375 - loss: 1.5676\n",
      "Epoch 4: val_accuracy did not improve from 0.44458\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.4375 - loss: 1.5676 - val_accuracy: 0.4438 - val_loss: 1.4901 - learning_rate: 1.0000e-04\n",
      "Epoch 5/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567ms/step - accuracy: 0.3412 - loss: 1.6963\n",
      "Epoch 5: val_accuracy improved from 0.44458 to 0.48838, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 623ms/step - accuracy: 0.3475 - loss: 1.6849 - val_accuracy: 0.4884 - val_loss: 1.3669 - learning_rate: 1.0000e-04\n",
      "Epoch 6/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:50\u001b[0m 493ms/step - accuracy: 0.3750 - loss: 1.6050\n",
      "Epoch 6: val_accuracy improved from 0.48838 to 0.49065, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.3750 - loss: 1.6050 - val_accuracy: 0.4907 - val_loss: 1.3643 - learning_rate: 1.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546ms/step - accuracy: 0.3725 - loss: 1.6325\n",
      "Epoch 7: val_accuracy improved from 0.49065 to 0.52751, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 659ms/step - accuracy: 0.3802 - loss: 1.6159 - val_accuracy: 0.5275 - val_loss: 1.2713 - learning_rate: 1.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:06\u001b[0m 398ms/step - accuracy: 0.4219 - loss: 1.4607\n",
      "Epoch 8: val_accuracy did not improve from 0.52751\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.4219 - loss: 1.4607 - val_accuracy: 0.5268 - val_loss: 1.2694 - learning_rate: 1.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m189/469\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:41\u001b[0m 576ms/step - accuracy: 0.3957 - loss: 1.5781\n",
      "Epoch 9: val_accuracy improved from 0.52751 to 0.55769, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 697ms/step - accuracy: 0.4039 - loss: 1.5617 - val_accuracy: 0.5577 - val_loss: 1.1984 - learning_rate: 1.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:00\u001b[0m 514ms/step - accuracy: 0.4531 - loss: 1.5702\n",
      "Epoch 10: val_accuracy did not improve from 0.55769\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.4531 - loss: 1.5702 - val_accuracy: 0.5529 - val_loss: 1.2043 - learning_rate: 1.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685ms/step - accuracy: 0.4155 - loss: 1.5203\n",
      "Epoch 11: val_accuracy improved from 0.55769 to 0.57786, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 803ms/step - accuracy: 0.4279 - loss: 1.5009 - val_accuracy: 0.5779 - val_loss: 1.1335 - learning_rate: 1.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:15\u001b[0m 418ms/step - accuracy: 0.4531 - loss: 1.4582\n",
      "Epoch 12: val_accuracy improved from 0.57786 to 0.57839, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.4531 - loss: 1.4582 - val_accuracy: 0.5784 - val_loss: 1.1334 - learning_rate: 1.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m198/469\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:28\u001b[0m 548ms/step - accuracy: 0.4384 - loss: 1.4709\n",
      "Epoch 13: val_accuracy improved from 0.57839 to 0.59335, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 687ms/step - accuracy: 0.4462 - loss: 1.4592 - val_accuracy: 0.5933 - val_loss: 1.1079 - learning_rate: 1.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:10\u001b[0m 407ms/step - accuracy: 0.4844 - loss: 1.4259\n",
      "Epoch 14: val_accuracy improved from 0.59335 to 0.59549, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.4844 - loss: 1.4259 - val_accuracy: 0.5955 - val_loss: 1.1068 - learning_rate: 1.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549ms/step - accuracy: 0.4675 - loss: 1.4175\n",
      "Epoch 15: val_accuracy improved from 0.59549 to 0.59776, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 575ms/step - accuracy: 0.4694 - loss: 1.4167 - val_accuracy: 0.5978 - val_loss: 1.0964 - learning_rate: 1.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 477ms/step - accuracy: 0.5469 - loss: 1.3375\n",
      "Epoch 16: val_accuracy did not improve from 0.59776\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5469 - loss: 1.3375 - val_accuracy: 0.5976 - val_loss: 1.0928 - learning_rate: 1.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516ms/step - accuracy: 0.4825 - loss: 1.3733\n",
      "Epoch 17: val_accuracy improved from 0.59776 to 0.62861, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 546ms/step - accuracy: 0.4799 - loss: 1.3751 - val_accuracy: 0.6286 - val_loss: 1.0158 - learning_rate: 1.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:15\u001b[0m 546ms/step - accuracy: 0.5156 - loss: 1.3769\n",
      "Epoch 18: val_accuracy improved from 0.62861 to 0.62967, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.5156 - loss: 1.3769 - val_accuracy: 0.6297 - val_loss: 1.0150 - learning_rate: 1.0000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - accuracy: 0.4966 - loss: 1.3512\n",
      "Epoch 19: val_accuracy improved from 0.62967 to 0.63462, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 686ms/step - accuracy: 0.4949 - loss: 1.3455 - val_accuracy: 0.6346 - val_loss: 0.9969 - learning_rate: 1.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:55\u001b[0m 504ms/step - accuracy: 0.5156 - loss: 1.2885\n",
      "Epoch 20: val_accuracy improved from 0.63462 to 0.63542, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.5156 - loss: 1.2885 - val_accuracy: 0.6354 - val_loss: 0.9974 - learning_rate: 1.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585ms/step - accuracy: 0.5028 - loss: 1.3216\n",
      "Epoch 21: val_accuracy improved from 0.63542 to 0.65425, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 692ms/step - accuracy: 0.5038 - loss: 1.3134 - val_accuracy: 0.6542 - val_loss: 0.9491 - learning_rate: 1.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:09\u001b[0m 534ms/step - accuracy: 0.5312 - loss: 1.2529\n",
      "Epoch 22: val_accuracy did not improve from 0.65425\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5312 - loss: 1.2529 - val_accuracy: 0.6525 - val_loss: 0.9510 - learning_rate: 1.0000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632ms/step - accuracy: 0.5204 - loss: 1.2858\n",
      "Epoch 23: val_accuracy improved from 0.65425 to 0.66627, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 692ms/step - accuracy: 0.5219 - loss: 1.2823 - val_accuracy: 0.6663 - val_loss: 0.9307 - learning_rate: 1.0000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:45\u001b[0m 611ms/step - accuracy: 0.5469 - loss: 1.2972\n",
      "Epoch 24: val_accuracy improved from 0.66627 to 0.66653, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5469 - loss: 1.2972 - val_accuracy: 0.6665 - val_loss: 0.9304 - learning_rate: 1.0000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663ms/step - accuracy: 0.5282 - loss: 1.2578\n",
      "Epoch 25: val_accuracy improved from 0.66653 to 0.67241, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 809ms/step - accuracy: 0.5313 - loss: 1.2607 - val_accuracy: 0.6724 - val_loss: 0.9104 - learning_rate: 1.0000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:40\u001b[0m 600ms/step - accuracy: 0.5469 - loss: 1.2152\n",
      "Epoch 26: val_accuracy did not improve from 0.67241\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.5469 - loss: 1.2152 - val_accuracy: 0.6707 - val_loss: 0.9130 - learning_rate: 1.0000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528ms/step - accuracy: 0.5424 - loss: 1.2343\n",
      "Epoch 27: val_accuracy improved from 0.67241 to 0.67735, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 557ms/step - accuracy: 0.5440 - loss: 1.2250 - val_accuracy: 0.6774 - val_loss: 0.8734 - learning_rate: 1.0000e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:42\u001b[0m 475ms/step - accuracy: 0.4844 - loss: 1.4419\n",
      "Epoch 28: val_accuracy improved from 0.67735 to 0.67882, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.4844 - loss: 1.4419 - val_accuracy: 0.6788 - val_loss: 0.8733 - learning_rate: 1.0000e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.5455 - loss: 1.2157\n",
      "Epoch 29: val_accuracy improved from 0.67882 to 0.68603, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 555ms/step - accuracy: 0.5492 - loss: 1.2080 - val_accuracy: 0.6860 - val_loss: 0.8516 - learning_rate: 1.0000e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:45\u001b[0m 483ms/step - accuracy: 0.6562 - loss: 1.0138\n",
      "Epoch 30: val_accuracy improved from 0.68603 to 0.68697, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.6562 - loss: 1.0138 - val_accuracy: 0.6870 - val_loss: 0.8500 - learning_rate: 1.0000e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602ms/step - accuracy: 0.5569 - loss: 1.1826\n",
      "Epoch 31: val_accuracy improved from 0.68697 to 0.68924, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 686ms/step - accuracy: 0.5588 - loss: 1.1827 - val_accuracy: 0.6892 - val_loss: 0.8450 - learning_rate: 1.0000e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:42\u001b[0m 603ms/step - accuracy: 0.5781 - loss: 1.0121\n",
      "Epoch 32: val_accuracy did not improve from 0.68924\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.5781 - loss: 1.0121 - val_accuracy: 0.6891 - val_loss: 0.8418 - learning_rate: 1.0000e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501ms/step - accuracy: 0.5718 - loss: 1.1578\n",
      "Epoch 33: val_accuracy improved from 0.68924 to 0.70152, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 561ms/step - accuracy: 0.5736 - loss: 1.1540 - val_accuracy: 0.7015 - val_loss: 0.8126 - learning_rate: 1.0000e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:46\u001b[0m 484ms/step - accuracy: 0.6094 - loss: 1.0535\n",
      "Epoch 34: val_accuracy improved from 0.70152 to 0.70166, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6094 - loss: 1.0535 - val_accuracy: 0.7017 - val_loss: 0.8121 - learning_rate: 1.0000e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524ms/step - accuracy: 0.5803 - loss: 1.1298\n",
      "Epoch 35: val_accuracy did not improve from 0.70166\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 553ms/step - accuracy: 0.5800 - loss: 1.1328 - val_accuracy: 0.6966 - val_loss: 0.8177 - learning_rate: 1.0000e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 478ms/step - accuracy: 0.7188 - loss: 0.9512\n",
      "Epoch 36: val_accuracy did not improve from 0.70166\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7188 - loss: 0.9512 - val_accuracy: 0.6952 - val_loss: 0.8158 - learning_rate: 1.0000e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517ms/step - accuracy: 0.5832 - loss: 1.1192\n",
      "Epoch 37: val_accuracy improved from 0.70166 to 0.71034, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 544ms/step - accuracy: 0.5819 - loss: 1.1212 - val_accuracy: 0.7103 - val_loss: 0.7975 - learning_rate: 1.0000e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 425ms/step - accuracy: 0.5312 - loss: 1.1171\n",
      "Epoch 38: val_accuracy improved from 0.71034 to 0.71074, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.5312 - loss: 1.1171 - val_accuracy: 0.7107 - val_loss: 0.7975 - learning_rate: 1.0000e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510ms/step - accuracy: 0.5849 - loss: 1.1050\n",
      "Epoch 39: val_accuracy improved from 0.71074 to 0.72009, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 540ms/step - accuracy: 0.5889 - loss: 1.1022 - val_accuracy: 0.7201 - val_loss: 0.7641 - learning_rate: 1.0000e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m  1/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:13\u001b[0m 542ms/step - accuracy: 0.6094 - loss: 1.0161\n",
      "Epoch 40: val_accuracy improved from 0.72009 to 0.72129, saving model to models/emotion_v2.keras\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.6094 - loss: 1.0161 - val_accuracy: 0.7213 - val_loss: 0.7621 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - accuracy: 0.6833 - loss: 0.8852\n",
      "Test accuracy after retrain: 0.6833133101463318\n",
      "Test loss after retrain: 0.8852333426475525\n"
     ]
    }
   ],
   "source": [
    "# ---------- improved_training.py ----------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "\n",
    "# Load your preprocessed arrays (already grayscale, 75x75)\n",
    "data = np.load(\"preprocessed_data/rafdb_preprocessed.npz\")\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val, y_val = data[\"X_val\"], data[\"y_val\"]\n",
    "X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "\n",
    "# If y_train are one-hot, convert to integer labels for class_weight calculation\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "\n",
    "# compute class weights (balanced)\n",
    "classes = np.unique(y_train_int)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_int)\n",
    "class_weight_dict = {int(c): w for c, w in zip(classes, class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# ---------------------------\n",
    "# Augmentation with CLAHE/brightness\n",
    "# ---------------------------\n",
    "def preprocessing_fn(img):\n",
    "    # img arrives as float32 in [0,1] by ImageDataGenerator, shape (h,w,channels)\n",
    "    # convert back to uint8 for cv operations, apply random CLAHE or brightness\n",
    "    img_uint8 = np.clip(img * 255.0, 0, 255).astype(np.uint8).squeeze()  # remove channel if single\n",
    "    # Randomly apply one of: none, CLAHE, brighten/darken\n",
    "    r = random.random()\n",
    "    if r < 0.25:\n",
    "        # CLAHE (adaptive equalization)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        out = clahe.apply(img_uint8)\n",
    "    elif r < 0.5:\n",
    "        # Brighten / darken\n",
    "        alpha = random.uniform(0.9, 1.2)  # contrast\n",
    "        beta = random.randint(-20, 30)    # brightness\n",
    "        out = cv2.convertScaleAbs(img_uint8, alpha=alpha, beta=beta)\n",
    "    else:\n",
    "        out = img_uint8\n",
    "    # convert back to float [0,1] and keep channel dim\n",
    "    out = out.astype(\"float32\") / 255.0\n",
    "    out = np.expand_dims(out, axis=-1)\n",
    "    return out\n",
    "\n",
    "# image data generator (we keep geometric aug similar to before but add preprocessing_fn)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    preprocessing_function=preprocessing_fn\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation generator (no aggressive augmentation, only rescale already applied)\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# Define model (same arch)\n",
    "# ---------------------------\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(75,75,1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "# callbacks\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "checkpoint = ModelCheckpoint(\"models/emotion_v2.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "early = EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train (more epochs; you can stop earlier via callbacks)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // batch_size,\n",
    "    epochs=40,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[checkpoint, reduce_lr, early]\n",
    ")\n",
    "\n",
    "# Evaluate on test set (best model will be saved)\n",
    "best = load_model(\"models/emotion_v2.keras\")\n",
    "test_loss, test_acc = best.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test accuracy after retrain:\", test_acc)\n",
    "print(\"Test loss after retrain:\", test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70529495-2891-4c62-955a-f49bd2799da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.7179 - loss: 0.7696\n",
      "\n",
      "✅ Test Accuracy: 0.7179\n",
      "Test Loss: 0.7696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = load_model(\"models/emotion_v2.keras\")\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"\\n✅ Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde32807-3297-4623-8890-a02150454ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.7179 - loss: 0.7696\n",
      "\n",
      "✅ Test Accuracy: 0.7179\n",
      "Test Loss: 0.7696\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(\"models/emotion_v2.keras\")\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"\\n✅ Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0631995-b131-4123-a57b-c194435a633d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
